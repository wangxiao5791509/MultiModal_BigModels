### Year 2024 


* **Uncertainty-Aware Evaluation for Vision-Language Models**, Vasily Kostumov, Bulat Nutfullin, Oleg Pilipenko, Eugene Ilyushin
  [[Paper](https://arxiv.org/abs/2402.14418)] 

* **MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms**, Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, Srijan Kumar
  [[Paper](https://arxiv.org/abs/2402.14154)]
  
* **Benchmarking Large Multimodal Models against Common Corruptions**, Jiawei Zhang * 1 Tianyu Pang 2 Chao Du 2 Yi Ren â€  3 Bo Li 1 4 Min Lin 2
  [[Paper](https://arxiv.org/pdf/2401.11943.pdf)]
  [[Code](https://github.com/sail-sg/MMCBench)]


### Year 2023 
* Li, Yifan, et al. "**Evaluating object hallucination in large vision-language models.**" arXiv preprint arXiv:2305.10355 (2023).
  [[Paper](https://arxiv.org/pdf/2305.10355.pdf)]
  [[Code](https://github.com/RUCAIBox/POPE)]

* Kamath, Amita, Jack Hessel, and Kai-Wei Chang. "**What's" up" with vision-language models? Investigating their struggle with spatial reasoning.**" arXiv preprint arXiv:2310.19785 (2023).
  [[Paper](https://arxiv.org/pdf/2310.19785.pdf)]
  [[Code](https://github.com/amitakamath/whatsup_vlms)] 
