* **Summary of related single- and multi-modal pre-training surveys.** SC and DC denotes Single Column and Double Column.   
   
| **Paper**     | **Link**           | **Year**           | **Publication**     | **Topic**          | **Pages** |
|:-----------   |:----------------:  |:----------------   |:----------------    |:----------------   |:----------------  |
| ***A short survey of pre-trained language models for conversational ai-a new age in nlp.*** <br />   | [[**Paper**]()]   | 2020  | ACSWM      | NLP   | DC, 4 | 
| ***A Survey of Controllable Text  Generation using Transformer-based  Pre-trained Language Models.*** <br />   | [[**Paper**]()] | 2022  | arXiv  | NLP | SC, 34| 
| ***A Survey of Knowledge  Enhanced Pre-trained Models.*** <br />      | [[**Paper**]()]      | 2021      | arXiv      | KE      | DC, 20	| 
| ***A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models.*** <br />      | [[**Paper**]()]      | 2022      | arXiv      | KE      | DC, 8	| 
| ***Commonsense Knowledge Reasoning  and Generation with Pre-trained Language Models: A Survey.*** <br />  | [[**Paper**]()]  | 2022  | arXiv  | KE    | DC, 11	| 
| ***A survey on contextual embeddings.*** <br />      | [[**Paper**]()]      | 2020      | arXiv      | NLP      | DC, 13	| 
| ***Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.*** <br />      | [[**Paper**]()]      | 2021      | arXiv      | NLP      | SC, 46	| 
| ***Pre-trained Language Models in Biomedical Domain: A Systematic Survey.*** <br />   | [[**Paper**]()]   |2021    |arXiv    |NLP    |SC, 46	|   
| ***Pre-trained models for natural language processing: A survey.*** <br />   | [[**Paper**]()]    |2020    |SCTS    |NLP    |DC, 26|
| ***Pre-Trained Models: Past, Present and Future.*** <br />   | [[**Paper**]()]     |2021    |AI Open    |NLP, CV, MM    |DC, 45|
| ***Recent Advances in Natural  Language Processing via Large Pre-Trained  Language Models: A Survey.*** <br />   | [[**Paper**]()]   |2021    |arXiv    |NLP    |DC, 49	   |
| ***A Survey of Vision-Language  Pre-Trained Models.*** <br />   | [[**Paper**]()]  |2022    |arXiv    |MM    |DC, 9	 |  
| ***Survey: Transformer based video-language pre-training.*** <br />   | [[**Paper**]()]  |2022    |AI Open   |CV    |DC, 13	   |
| ***Vision-Language Intelligence: Tasks, Representation Learning, and Large Models.*** <br />   | [[**Paper**]()]     |2022    |arXiv    |MM    |DC, 19|	   
| ***A survey on vision transformer.*** <br />   | [[**Paper**]()] |2022    |TPAMI    |CV    |DC, 23| 
| ***Transformers in vision: A survey.*** <br />   | [[**Paper**]()]     |2021    |CSUR    |CV    |SC, 38|
| ***A Survey of Visual Transformers.*** <br />   | [[**Paper**]()]     |2021    |arXiv    |CV    |DC, 21|
| ***Video Transformers: A Survey.*** <br />   | [[**Paper**]()]      |2022    |arXiv    |CV    |DC, 24 |
| ***Threats to Pre-trained Language Models: Survey and Taxonomy.*** <br />   | [[**Paper**]()]     |2022    |arXiv    |NLP    |DC, 8 |
| ***A survey on bias in deep NLP.*** <br />   | [[**Paper**]()]    |2021    |AS    |NLP    |SC, 26	|   
| ***A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models.*** <br />   | [[**Paper**]()] |2022    |arXiv    |NLP    |SC, 34|	   
| ***An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-Trained Language Models.*** <br />   | [[**Paper**]()]     |2021    |arXiv    |NLP    |DC, 21| 
| ***A multi-layer bidirectional transformer  encoder for pre-trained word embedding: A survey of BERT.*** <br />   | [[**Paper**]()]    |2020    |CCDSE    |NLP    |DC, 5	|
| ***Survey of Pre-trained Models for Natural Language Processing.*** <br />   | [[**Paper**]()]   |2021    |ICEIB    |NLP    |DC, 4|
| ***A Roadmap for Big Model.*** <br />   | [[**Paper**]()]  |2022    |arXiv    |NLP, CV, MM    |SC, 200   | 
| ***Vision-and-Language Pretrained Models: A Survey.*** <br />   | [[**Paper**]()]  |2022    |IJCAI    |MM    |DC, 8   |
| ***Multimodal Learning with Transformers: A Survey.*** <br />   | [[**Paper**]()]  |2022    |arXiv    |MM    |DC, 23   |






