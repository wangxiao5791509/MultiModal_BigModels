* **Summary of related single- and multi-modal pre-training surveys.** SC and DC denotes Single Column and Double Column.   
   
| **Paper**     | **Link**           | **Year**           | **Publication**     | **Topic**          | **Pages** |
|:-----------   |:----------------:  |:----------------   |:----------------    |:----------------   |:----------------  |
| ***[01] A short survey of pre-trained language models for conversational ai-a new age in nlp.*** <br />   | [[**Paper**](https://dl.acm.org/doi/pdf/10.1145/3373017.3373028?casa_token=k6huQ7_0BfIAAAAA:qzfR7SRNeLmylyRxS0M31LvvYX75svCbUy2PFUw5sNU443z8Z50m2ALwympsDQYxuo57xhfrEeodWaQ)]   | 2020  | ACSWM      | NLP   | DC, 4 | 
| ***[02] A Survey of Controllable Text  Generation using Transformer-based  Pre-trained Language Models.*** <br />   | [[**Paper**](https://arxiv.org/abs/2201.05337)] | 2022  | arXiv  | NLP | SC, 34| 
| ***[03] A Survey of Knowledge  Enhanced Pre-trained Models.*** <br />      | [[**Paper**](https://arxiv.org/abs/2110.00269)]      | 2021      | arXiv      | KE      | DC, 20	| 
| ***[04] A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models.*** <br />      | [[**Paper**](https://arxiv.org/abs/2202.08772)]      | 2022      | arXiv      | KE      | DC, 8	| 
| ***[05] Commonsense Knowledge Reasoning  and Generation with Pre-trained Language Models: A Survey.*** <br />  | [[**Paper**](https://www.aaai.org/AAAI22Papers/SMT-00456-BhargavaP.pdf)]  | 2022  | arXiv  | KE    | DC, 11	| 
| ***[06] A survey on contextual embeddings.*** <br />      | [[**Paper**](https://arxiv.org/abs/2003.07278)]      | 2020      | arXiv      | NLP      | DC, 13	| 
| ***[07] Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.*** <br />      | [[**Paper**](https://arxiv.org/abs/2107.13586)]      | 2021      | arXiv     | NLP      | SC, 46	| 
| ***[08] Pre-trained Language Models in Biomedical Domain: A Systematic Survey.*** <br />   | [[**Paper**](https://arxiv.org/abs/2110.05006)]   |2021    |arXiv    |NLP    |SC, 46	|   
| ***[09] Pre-trained models for natural language processing: A survey.*** <br />   | [[**Paper**](https://link.springer.com/article/10.1007/s11431-020-1647-3)]    |2020    |SCTS    |NLP    |DC, 26|
| ***[10] Pre-Trained Models: Past, Present and Future.*** <br />   | [[**Paper**](https://www.sciencedirect.com/science/article/pii/S2666651021000231)]     |2021    |AI Open    |NLP, CV, MM    |DC, 45|
| ***[11] Recent Advances in Natural  Language Processing via Large Pre-Trained  Language Models: A Survey.*** <br />   | [[**Paper**](https://arxiv.org/abs/2111.01243)]   |2021    |arXiv    |NLP    |DC, 49	   |
| ***[12] A Survey of Vision-Language  Pre-Trained Models.*** <br />   | [[**Paper**](https://arxiv.org/abs/2202.10936)]  |2022    |arXiv    |MM    |DC, 9	 |  
| ***[13] Survey: Transformer based video-language pre-training.*** <br />   | [[**Paper**](https://www.sciencedirect.com/science/article/pii/S2666651022000018)]  |2022    |AI Open   |CV    |DC, 13	   |
| ***[14] Vision-Language Intelligence: Tasks, Representation Learning, and Large Models.*** <br />   | [[**Paper**](https://arxiv.org/abs/2203.01922)]     |2022    |arXiv    |MM    |DC, 19|	   
| ***[15] A survey on vision transformer.*** <br />   | [[**Paper**](https://ieeexplore.ieee.org/abstract/document/9716741/)] |2022    |TPAMI    |CV    |DC, 23| 
| ***[16] Transformers in vision: A survey.*** <br />   | [[**Paper**](https://dl.acm.org/doi/abs/10.1145/3505244?casa_token=w1xQ4QLa3SAAAAAA:hGG_nZjfKVIKdq1H-uEdY70yxrFG5dx_PjgMT1-YiC4mhhKFlOx8eIqaiFRtDr7K2bv716F3GHhlBvw)]     |2021    |CSUR    |CV    |SC, 38|
| ***[17] A Survey of Visual Transformers.*** <br />   | [[**Paper**](https://arxiv.org/abs/2111.06091)]     |2021    |arXiv    |CV    |DC, 21|
| ***[18] Video Transformers: A Survey.*** <br />   | [[**Paper**](https://arxiv.org/abs/2201.05991)]      |2022    |arXiv    |CV    |DC, 24 |
| ***[19] Threats to Pre-trained Language Models: Survey and Taxonomy.*** <br />   | [[**Paper**](https://arxiv.org/abs/2202.06862)]     |2022    |arXiv    |NLP    |DC, 8 |
| ***[20] A survey on bias in deep NLP.*** <br />   | [[**Paper**](https://www.mdpi.com/2076-3417/11/7/3184/htm?ref=https://githubhelp.com)]    |2021    |AS    |NLP    |SC, 26	|   
| ***[21] A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models.*** <br />   | [[**Paper**](https://arxiv.org/abs/2201.05337)] |2022    |arXiv    |NLP    |SC, 34|	   
| ***[22] An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-Trained Language Models.*** <br />   | [[**Paper**](https://arxiv.org/abs/2110.08527)]     |2021    |arXiv    |NLP    |DC, 21| 
| ***[23] A multi-layer bidirectional transformer  encoder for pre-trained word embedding: A survey of BERT.*** <br />   | [[**Paper**](https://ieeexplore.ieee.org/abstract/document/9058044/)]    |2020    |CCDSE    |NLP    |DC, 5	|
| ***[24] Survey of Pre-trained Models for Natural Language Processing.*** <br />   | [[**Paper**](https://ieeexplore.ieee.org/abstract/document/9686420/)]   |2021    |ICEIB    |NLP    |DC, 4|
| ***[25] A Roadmap for Big Model.*** <br />   | [[**Paper**](https://arxiv.org/abs/2203.14101)]  |2022    |arXiv    |NLP, CV, MM    |SC, 200   | 
| ***[26] Vision-and-Language Pretrained Models: A Survey.*** <br />   | [[**Paper**](https://arxiv.org/abs/2204.07356)]  |2022    |IJCAI    |MM    |DC, 8   |
| ***[27] Multimodal Learning with Transformers: A Survey.*** <br />   | [[**Paper**](https://arxiv.org/abs/2206.06488)]  |2022    |arXiv    |MM    |DC, 23   |
| ***[28] MM-LLMs: Recent Advances in MultiModal Large Language Models.*** <br />   | [[**Paper**](https://arxiv.org/pdf/2401.13601.pdf)]  |2024    |arXiv    |MM    |DC, 22  |


* **The (R)Evolution of Multimodal Large Language Models: A Survey**, Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Lorenzo Baraldi, Marcella Cornia, Rita Cucchiara
  [[Paper](https://arxiv.org/abs/2402.12451)]

* **Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models**, Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, Lichao Sun
  [[Paper](https://arxiv.org/abs/2402.17177)]

* **On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models**,
  Xinpeng Wang, Shitong Duan, Xiaoyuan Yi, Jing Yao, Shanlin Zhou, Zhihua Wei, Peng Zhang, Dongkuan Xu, Maosong Sun, Xing Xie
  [[Paper](https://arxiv.org/abs/2403.04204)] 

* [arXiv:2405.10739] **Efficient Multimodal Large Language Models: A Survey**, 
  Yizhang Jin, Jian Li, Yexin Liu, Tianjun Gu, Kai Wu, Zhengkai Jiang, Muyang He, Bo Zhao, Xin Tan, Zhenye Gan, Yabiao Wang, Chengjie Wang, Lizhuang Ma
  [[Paper](https://arxiv.org/abs/2405.10739)]
  [[Code](https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey)] 









