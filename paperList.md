* **The summary of mainstream multi-modal pre-trained big models.** 

### Year 2019 and Before 
|**NO.**     | **Model**     | **Publish**        | **Modality**        | **Architecture**   | **Objective**     |**Highlights**   |**Code**         |
|:-----------|:-----------   |:----------------   |:----------------    |:----------------   |:----------------  |:----------------|:----------------|
|01 |VisualBERT |arXiv-2019 |image-text |Trans, BERT |GR, MML |A simple and strong baseline for VLP |[[Code](https://github.com/uclanlp/visualbert)]
|02 |ViLBERT |NeurIPS-2019 |image-text |Trans |CS, GR |First adopt co-attention for MM pre-training |[[Code](https://github.com/jiasenlu/vilbert_beta)] 
|03 |LXMERT |EMNLP-2019 |image-text |Trans | QA, MOR, MOC, MML, MLM |Propose a cross-modality encoder for vision-language pre-training |[[Code](https://github.com/airsplay/lxmert)] 
|04 |B2T2 |EMNLP-2019 |image-text |ResNet, BERT |MML, GR |Embed bounding box into text transformer in a early fusion manner |[[Code](https://github.com/google-research/language/tree/master/language/question_answering/b2t2)]  
|06 |VL-BERT |ICLR-2019 |image-text |BERT |GR, MOC |MM PTMs and faster rcnn are jointly trained |[[Code](https://github.com/jackroos/VL-BERT)] 




### Year 2020 
|**NO.**     | **Model**     | **Publish**        | **Modality**        | **Architecture**   | **Objective**     |**Highlights**   |**Code**         |
|:-----------|:-----------   |:----------------   |:----------------    |:----------------   |:----------------  |:----------------|:----------------|
|05 |Unicoder-VL |AAAI-2020 |image-text |Trans |GR, MML, MOC |Single transformer encoder for VLP |[[Code](https://github.com/microsoft/Unicoder)]
|07 |VLP |AAAI-2020 |image-text |Trans |BiDT, Seq2seq |Unified encoder-decoder network architecture |[[Code](https://github.com/LuoweiZhou/VLP)] 
|08 |UNITER |ECCV-2020 |image-text |Trans |MRA, MML |Propose an OT-based Word-Region Alignment objective |[[Code](https://github.com/ChenRocks/UNITER)]  
|09 |12-IN-1  |CVPR-2020  |image-text |Trans |CS, GR |Training jointly on 12 different datasets in a multi-task learning manner |[[Code](https://github.com/facebookresearch/vilbert-multi-task)] 
|10 |VisDial-BERT |ECCV-2020 |image-text |Trans |MLM, NSP, MIR |Pre-training on image-text corpus and finetuning on visual dialog |[[Code](https://github.com/vmurahari3/visdial-bert/)]
|11 |ImageBERT |arXiv-2020 |image-text |Trans |MOC, MLM, MML, MOR |Indicating that multi-stage pre-training works better |-  
|12 |PREVALENT |CVPR-2020 |image-text |Trans |MLM, AP |Pre-training for vision and language navigation |[[Code](https://github.com/weituo12321/PREVALENT)] 



















































































