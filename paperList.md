* **The summary of mainstream multi-modal pre-trained big models.** 




### Year 2024 


* [arXiv:2412.00832] **EventGPT: Event Stream Understanding with Multimodal Large Language Models**,
  Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Xin Meng, Fei Richard Yu, Xiangyang Ji, Ming Li
  [[Paper](https://arxiv.org/abs/2412.00832)]
  
* **BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices**,
  Xudong Lu, Yinghao Chen, Cheng Chen, Hui Tan, Boheng Chen, Yina Xie, Rui Hu, Guanxin Tan, Renshou Wu, Yan Hu, Yi Zeng, Lei Wu, Liuyang Bian, Zhaoxiong Wang, Long Liu, Yanzhou Yang, Han Xiao, Aojun Zhou, Yafei Wen, Xiaoxin Chen, Shuai Ren, Hongsheng Li
  [[Paper](https://arxiv.org/abs/2411.10640)] 

* [arXiv:2410.21276] **GPT-4o System Card,** OpenAI: Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Mądry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn et al.
  [[Paper](https://arxiv.org/abs/2410.21276)] 

* [arXiv:2410.18927] Ying, Zonghao, et al. "**SafeBench: A Safety Evaluation Framework for Multimodal Large Language Models**." arXiv preprint arXiv:2410.18927 (2024).
  [[Paper](https://arxiv.org/abs/2410.18927)] 

* [arXiv:2410.18857] **Probabilistic Language-Image Pre-Training**,
  Sanghyuk Chun, Wonjae Kim, Song Park, Sangdoo Yun
  [[Paper](https://arxiv.org/abs/2410.18857)] 

* [**Liquid Foundation Models: Our First Series of Generative AI Models**](https://www.liquid.ai/liquid-foundation-models)
  [[Github](https://github.com/kyegomez/LFM)]
  [[https://www.liquid.ai/liquid-foundation-models](https://www.liquid.ai/liquid-foundation-models)]
  [[Blog](https://hereiskunalverma.medium.com/liquid-foundation-models-next-frontier-of-ai-df394f51fc7a)]
  [[From Liquid Neural Networks to Liquid Foundation Models](https://www.liquid.ai/blog/liquid-neural-networks-research)]
  [[https://playground.liquid.ai/chat](https://playground.liquid.ai/chat)]
  [[Liquid Foundation Models - 3 LLMs (1B, 3B, 40B) on Custom Architecture-Youtube](https://www.youtube.com/watch?v=l3M0DXU0UjM&ab_channel=FahdMirza)]
  
* **Emu3: Next-Token Prediction is All You Need**, Emu3 Team, BAAI
  [[Paper](https://arxiv.org/pdf/2409.18869)] 

* [arXiv:2409.18119] **Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography**,
  Yuexi Du, John Onofrey, Nicha C. Dvornek
  [[Paper](https://arxiv.org/abs/2409.18119)] 

* [arXiv:2409.18111] **E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding**,
  Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, Chang Wen Chen
  [[Paper](https://arxiv.org/abs/2409.18111)] 

* **EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions**,
  Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James T. Kwok, Hengshuang Zhao, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo Li, Wei Zhang, Qun Liu, Lanqing Hong, Lu Hou, Hang Xu
  [[Paper](https://arxiv.org/abs/2409.18042)] 

* [arXiv:2409.17146] **Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models**, Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi
  [[Paper](https://arxiv.org/abs/2409.17146)]
  [[Code](https://molmo.allenai.org/)] 

* [arXiv:2409.12568] **InfiMM-WebMath-40B: Advancing Multimodal Pre-Training for Enhanced Mathematical Reasoning**,
  Xiaotian Han, Yiren Jian, Xuefeng Hu, Haogeng Liu, Yiqi Wang, Qihang Fan, Yuang Ai, Huaibo Huang, Ran He, Zhenheng Yang, Quanzeng You
  [[Paper](https://arxiv.org/abs/2409.12568)] 
  
* **General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model**,
  Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, Chunrui Han, Xiangyu Zhang
  [[Paper](https://arxiv.org/abs/2409.01704)] 
  
* [arXiv:2408.16500] **CogVLM2: Visual Language Models for Image and Video Understanding**,
  Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, Lei Zhao, Zhuoyi Yang, Xiaotao Gu, Xiaohan Zhang, Guanyu Feng, Da Yin, Zihan Wang, Ji Qi, Xixuan Song, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Yuxiao Dong, Jie Tang 
  [[Paper](https://arxiv.org/abs/2408.16500)]
  
* [arXiv:2408.14471] **A Practitioner's Guide to Continual Multimodal Pretraining**, 
  Karsten Roth, Vishaal Udandarao, Sebastian Dziadzio, Ameya Prabhu, Mehdi Cherti, Oriol Vinyals, Olivier Hénaff, Samuel Albanie, Matthias Bethge, Zeynep Akata 
  [[Paper](https://arxiv.org/abs/2408.14471)]
  [[Code](https://github.com/ExplainableML/fomo_in_flux)] 
  
* [arXiv:2408.08872] **xGen-MM (BLIP-3): A Family of Open Large Multimodal Models**, Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael S Ryoo, Shrikant Kendre, Jieyu Zhang, Can Qin, Shu Zhang, Chia-Chih Chen, Ning Yu, Juntao Tan, Tulika Manoj Awalgaonkar, Shelby Heinecke, Huan Wang, Yejin Choi, Ludwig Schmidt, Zeyuan Chen, Silvio Savarese, Juan Carlos Niebles, Caiming Xiong, Ran Xu
[[Paper](https://arxiv.org/abs/2408.08872)]
[[Project](https://github.com/salesforce/LAVIS/tree/xgen-mm)] 

* **VITA: Towards Open-Source Interactive Omni Multimodal LLM**,
  Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Xiong Wang, Di Yin, Long Ma, Xiawu Zheng, Ran He, Rongrong Ji, Yunsheng Wu, Caifeng Shan, Xing Sun
  [[Paper](https://arxiv.org/abs/2408.05211)] 
  
* [arXiv:2408.04840] **mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models**,
  Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou
  https://arxiv.org/abs/2408.04840
  
* [arXiv:2408.02718] **MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models**,
  Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, Kaipeng Zhang, Wenqi Shao
  [[Paper](https://arxiv.org/abs/2408.02718)] 

* [arXiv:2408.02865] **VisionUnite: A Vision-Language Foundation Model for Ophthalmology Enhanced with Clinical Knowledge**,
  Zihan Li, Diping Song, Zefeng Yang, Deming Wang, Fei Li, Xiulan Zhang, Paul E. Kinahan, Yu Qiao
  [[Paper](https://arxiv.org/abs/2408.02865)] 
  
* [arXiv:2408.03326] **LLaVA-OneVision: Easy Visual Task Transfer**,
  Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li
  [[Paper](https://arxiv.org/abs/2408.03326)]
  [[Code](https://llava-vl.github.io/blog/2024-08-05-llava-onevision/)] 
  
* [arXiv:2407.14885] **Falcon2-11B Technical Report**, Quentin Malartic, Nilabhra Roy Chowdhury, Ruxandra Cojocaru, Mugariya Farooq, Giulia Campesan, Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Ankit Singh, Maksim Velikanov, Basma El Amel Boussaha, Mohammed Al-Yafeai, Hamza Alobeidli, Leen Al Qadi, Mohamed El Amine Seddik, Kirill Fedyanin, Reda Alami, Hakim Hacid
  [[Paper](https://arxiv.org/abs/2407.14885)]
  [[huggingface](https://huggingface.co/tiiuae)]

* [CVPR 2024] **Improved Baselines with Visual Instruction Tuning**, Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee
  [[Paper](https://arxiv.org/abs/2310.03744)]
  [[Code](https://llava-vl.github.io/)]
  
* [arXiv:2407.14177] **EVLM: An Efficient Vision-Language Model for Visual Understanding**, Kaibing Chen, Dong Shen, Hanwen Zhong, Huasong Zhong, Kui Xia, Di Xu, Wei Yuan, Yifei Hu, Bin Wen, Tianke Zhang, Changyi Liu, Dewen Fan, Huihui Xiao, Jiahong Wu, Fan Yang, Size Li, Di Zhang
  [[Paper](https://arxiv.org/abs/2407.14177)] 

* [arXiv:2407.07726] **PaliGemma: A versatile 3B VLM for transfer**, Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bošnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, Xiaohua Zhai
  [[Paper](https://arxiv.org/abs/2407.07726)]
  
* [arXiv:2407.03418] **HEMM: Holistic Evaluation of Multimodal Foundation Models**, Paul Pu Liang, Akshay Goindani, Talha Chafekar, Leena Mathur, Haofei Yu, Ruslan Salakhutdinov, Louis-Philippe Morency
  [[Paper](https://arxiv.org/abs/2407.03418)]
  [[Code](https://github.com/pliang279/HEMM)]
  
* [arXiv:2406.11832] **Unveiling Encoder-Free Vision-Language Models**, Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, Xinlong Wang
  [[Paper](https://arxiv.org/abs/2406.11832)] 

* **Pegasus-v1 Technical Report**, arXiv:2404.14687, Raehyuk Jung, Hyojun Go, Jaehyuk Yi, Jiho Jang, Daniel Kim, Jay Suh, Aiden Lee, Cooper Han, Jae Lee, Jeff Kim, Jin-Young Kim, Junwan Kim, Kyle Park, Lucas Lee, Mars Ha, Minjoon Seo, Abraham Jo, Ed Park, Hassan Kianinejad, SJ Kim, Tony Moon, Wade Jeong, Andrei Popescu, Esther Kim, EK Yoon, Genie Heo, Henry Choi, Jenna Kang, Kevin Han, Noah Seo, Sunny Nguyen, Ryan Won, Yeonhoo Park, Anthony Giuliani, Dave Chung, Hans Yoon, James Le, Jenny Ahn, June Lee, Maninder Saini, Meredith Sanders, Soyoung Lee, Sue Kim, Travis Couture
[[Paper](https://arxiv.org/abs/2404.14687)] 

* **MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training**, 
  Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang
  [[Paper](https://arxiv.org/abs/2403.09611)] 

* **EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters**, Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Xinlong Wang
  [[Paper](https://arxiv.org/abs/2402.04252)]
  [[Code](https://github.com/baaivision/EVA/tree/master/EVA-CLIP-18B)] 

* **InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Models**, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang 
  [[Paper](https://arxiv.org/pdf/2401.16420.pdf)]
  [[Code](https://github.com/InternLM/InternLM-XComposer)] 






### Year 2023 


* [arXiv:2310.07704] **Ferret: Refer and Ground Anything Anywhere at Any Granularity**,
  Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang
  [[Paper](https://arxiv.org/abs/2310.07704)]
  [[Code](https://github.com/apple/ml-ferret)] 

* [LLaVA] **Visual Instruction Tuning**, Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, NeurIPS 2023 Oral 
  [[Paper](https://arxiv.org/abs/2304.08485)]
  [[Code](https://llava-vl.github.io/)] 

* **PALI-3 VISION LANGUAGE MODELS: SMALLER, FASTER, STRONGER**,
[[Paper](https://arxiv.org/pdf/2310.09199.pdf)]

* **Fuyu-8B: A Multimodal Architecture for AI Agents**, [[https://www.adept.ai/blog/fuyu-8b](https://www.adept.ai/blog/fuyu-8b)]

* **OtterHD: A High-Resolution Multi-modality Model**, Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, Ziwei Liu 
[[Paper](https://arxiv.org/pdf/2311.04219.pdf)]
[[Code](https://github.com/Luodian/Otter)]



* **Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining**, Bingqian Lin et al. [[Paper](https://arxiv.org/pdf/2304.14204.pdf)]

* **CLIP^2: Contrastive Language-Image-Point Pretraining from Real-World Point Cloud Data**, Yihan Zeng, Chenhan Jiang, Jiageng Mao, Jianhua Han, Chaoqiang Ye, Qingqiu Huang, Dit-Yan Yeung, Zhen Yang, Xiaodan Liang, Hang Xu 
[[Paper](https://arxiv.org/pdf/2303.12417.pdf)]

* **PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents**, Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie, [[Paper](https://arxiv.org/pdf/2303.07240.pdf)]

* **HICLIP: CONTRASTIVE LANGUAGE-IMAGE PRETRAINING WITH HIERARCHY-AWARE ATTENTION**, Shijie Geng, Jianbo Yuan, Yu Tian, Yuxiao Chen, Yongfeng Zhang, ICLR 2023 
[[Paper](https://arxiv.org/pdf/2303.02995.pdf)] 
[[Code](https://github.com/jeykigung/HiCLIP)]

* **FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks**, Xiao Han, Xiatian Zhu, Licheng Yu, Li Zhang, Yi-Zhe Song, Tao Xiang
[[Paper](https://arxiv.org/pdf/2303.02483.pdf)] 
[[Code](https://github.com/BrandonHanx/FAME-ViL)]

* **Prismer: A Vision-Language Model with An Ensemble of Experts**, Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei Xiao, Anima Anandkumar
[[Paper](https://arxiv.org/pdf/2303.02506.pdf)] 
[[Code](https://github.com/NVlabs/prismer)]

* **STRUCTEXTV2: MASKED VISUAL-TEXTUAL PREDICTION FOR DOCUMENT IMAGE PRE-TRAINING**, Yuechen Yu, Yulin Li, Chengquan Zhang, Xiaoqiang Zhang, Zengyuan Guo,Xiameng Qin, Kun Yao, Junyu Han, Errui Ding, Jingdong Wang
[[Paper](https://arxiv.org/pdf/2303.00289.pdf)] 
[[Code](https://github.com/PaddlePaddle/VIMER/tree/main/StrucTexT)]

* **Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training**, Dezhao Luo, Jiabo Huang, Shaogang Gong, Hailin Jin, and Yang Liu [[Paper](https://arxiv.org/pdf/2303.00040.pdf)] 

* **RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training**, Zheng Yuan, Qiao Jin12, Chuanqi Tan, Zhengyun Zhao, Hongyi Yuan, Fei Huang, Songfang Huang [[Paper](https://arxiv.org/pdf/2303.00534.pdf)]

* "**Language Is Not All You Need: Aligning Perception with Language Models.**" arXiv preprint arXiv:2302.14045 (2023). Huang, Shaohan, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv et al. 
[[Paper](https://arxiv.org/pdf/2302.14045.pdf)]
[[Code](https://github.com/microsoft/unilm)]

* **Improving Medical Speech-to-Text Accuracy with Vision-Language Pre-training Model**, Jaeyoung Huha, Sangjoon Parka, Jeong Eun Leeb, Jong Chul Ye 
[[Paper](https://arxiv.org/pdf/2303.00091.pdf)]

* **Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning**, CVPR 2023, Yang, Antoine and Nagrani, Arsha and Seo, Paul Hongsuck and Miech, Antoine and Pont-Tuset, Jordi and Laptev, Ivan and Sivic, Josef and Schmid, Cordelia, 
[[Paper](https://arxiv.org/pdf/2302.14115.pdf)]
[[Project](https://antoyang.github.io/vid2seq.html)]

* **Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images**, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie, 
[[arXiv](https://arxiv.org/pdf/2302.14042.pdf)]



### Year 2022 

* **FLAVA: A Foundational Language And Vision Alignment Model**, Amanpreet Singh* Ronghang Hu* Vedanuj Goswami* Guillaume Couairon Wojciech Galuba Marcus Rohrbach Douwe Kiela, CVPR_2022 
[[Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.pdf)]
[[Project](https://flava-model.github.io/)] 
[[Code](https://github.com/facebookresearch/multimodal/tree/main/examples/flava)]
  
* **Position-guided Text Prompt for Vision-Language Pre-training**, Alex Jinpeng Wang, Pan Zhou, Mike Zheng Shou, Shuicheng Yan 
[[Paper](https://arxiv.org/abs/2212.09737)] 
[[Code](https://github.com/sail-sg/ptp)]

* **MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training**, Chaoyi Wu1,2, Xiaoman Zhang1,2, Ya Zhang1,2, Yanfeng Wang1,2, Weidi Xie 
[[Paper](https://arxiv.org/pdf/2301.02228.pdf)] 
[[Code](https://chaoyi-wu.github.io/MedKLIP/)]

* **Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained Vision-Language Models**, Wenhao Wu1,2 Xiaohan Wang3 Haipeng Luo4 Jingdong Wang1 Yi Yang3 Wanli Ouyang 
[[Paper](https://arxiv.org/pdf/2301.00182.pdf)]


* **HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training**, Qinghao Ye Guohai Xu Ming Yan∗ Haiyang Xu Qi Qian Ji Zhang Fei Huang, 
[[Paper](https://arxiv.org/pdf/2212.14546.pdf)] 
[[Model](https://www.modelscope.cn/home)]

* **Million-scale Object Detection with Large Vision Model**, Feng Lin, Wenze Hu, Yaowei Wang, Yonghong Tian, Guangming Lu, Fanglin, Chen, Yong Xu and Xiaoyu Wang, 
[[Paper](https://arxiv.org/pdf/2212.09408.pdf)]
[[Code](https://github.com/linfeng93/Large-UniDet)]


* **VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning**, Qiushi Zhu, Long Zhou, Ziqiang Zhang, Shujie Liu, Binxing Jiao, Jie Zhang, Lirong Dai, Daxin Jiang, Jinyu Li, Furu Wei
[[Paper](https://arxiv.org/pdf/2211.11275.pdf)]
[[Code](https://github.com/microsoft/SpeechT5/tree/main/VATLM)]

* **SIMLA: Single-Stream Multi-Level Alignment for Vision-Language Pretraining**, ECCV 2022 (NEC Labs), Zaid Khan, Vijay Kumar, Xiang Yu, Samuel Schulter, Manmohan Chandraker, and Yun Fu
[[Paper](https://arxiv.org/pdf/2203.14395.pdf)]
[[Code](https://github.com/codezakh/SIMLA)]
[[Project](http://zaidkhan.me/SIMLA/)]

* **VINDLU : A Recipe for Effective Video-and-Language Pretraining**, Feng Cheng, Xizi Wang, Jie Lei, David Crandall, Mohit Bansal, Gedas Bertasius 
[[Paper](https://arxiv.org/pdf/2212.05051.pdf)] 
[[Code](https://github.com/klauscc/VindLU)]


* **CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet**, Xiaoyi Dong*, Jianmin Bao, Ting Zhang, Dongdong Chen, Shuyang Gu, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, Nenghai Yu 
[[Paper](https://arxiv.org/pdf/2212.06138.pdf)] 
[[Code](https://github.com/LightDXY/FT-CLIP)]

* **REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory**, Ziniu Hu1*, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A. Ross, Alireza Fathi 
[[Paper](https://arxiv.org/pdf/2212.05221.pdf)] 


* **Scaling Multimodal Pre-Training via Cross-Modality Gradient Harmonization**, Junru Wu et al. 
[[Paper](https://arxiv.org/pdf/2211.02077.pdf)] 

* **Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese**, An Yang et al. 
[[Paper](https://arxiv.org/pdf/2211.01335.pdf)] 
[[Code](https://github.com/OFA-Sys/Chinese-CLIP)]

* **Generative Negative Text Replay for Continual Vision-Language Pretraining**, [[Paper](https://arxiv.org/pdf/2210.17322.pdf)]

* **GRIT-VLP: GRouped mIni-baTch sampling for Efficient Vision-Language Pre-training**, ECCV 2022, 
[[Paper](https://arxiv.org/abs/2208.04060)]
[[Code](GRIT-VLP: GRouped mIni-baTch sampling for Efficient Vision-Language Pre-training)]

* **INSTRUCTION-FOLLOWING AGENTS WITH JOINTLY PRE-TRAINED VISION-LANGUAGE MODELS**, Hao Liu et al. 
[[Paper](https://arxiv.org/pdf/2210.13431.pdf)] 
[[Code](https://github.com/lhao499/instructrl)]

* **FaD-VLP: Fashion Vision-and-Language Pre-training towards Unified Retrieval and Captioning**, Suvir Mirchandani, et al. 
[[Paper](https://arxiv.org/pdf/2210.15028.pdf)]

* **ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts**, Zhida Feng et al. 
[[Paper](https://arxiv.org/pdf/2210.15257.pdf)]

* **Learning by Hallucinating: Vision-Language Pre-training with Weak Supervision**, 
[[Paper](https://arxiv.org/pdf/2210.13591.pdf)] 

* **MedCLIP: Contrastive Learning from Unpaired Medical Images and Text**, Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, Jimeng Sun 
[[Paper](https://arxiv.org/pdf/2210.10163.pdf)]
[[Code](https://github.com/RyanWangZf/MedCLIP)]
 
* **Contrastive Language-Image Pre-Training with Knowledge Graphs**, 
[[Paper](https://arxiv.org/pdf/2210.08901.pdf)]

* **Vision-Language Pre-training: Basics, Recent Advances, and Future Trends**, Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao 
[[Paper](https://arxiv.org/pdf/2210.09263.pdf)] 
[[Recent Advances in Vision-and-Language Pre-training In conjunction with CVPR 2022](https://vlp-tutorial.github.io/)] 

* **Non-Contrastive Learning Meets Language-Image Pre-Training**, Jinghao Zhou Li Dong Zhe Gan Lijuan Wang Furu Wei 
[[Paper](https://arxiv.org/pdf/2210.09304.pdf)] 

* **Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training**, Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, Pascale Fung [[Paper](https://arxiv.org/pdf/2210.07688.pdf)]

* **Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning**, Yuchong Sun, Hongwei Xue, Ruihua Song, Bei Liu, Huan Yang, Jianlong Fu, [[Paper](https://arxiv.org/pdf/2210.06031.pdf)] [[Code](https://github.com/microsoft/XPretrain)]

* **MAP: Modality-Agnostic Uncertainty-Aware Vision-Language Pre-training Model**, Yatai Ji Junjie Wang Yuan Gong Lin Zhang Yanru Zhu Hongfa Wang Jiaxing Zhang Tetsuya Sakai Yujiu Yang, [[Paper](https://arxiv.org/pdf/2210.05335.pdf)] 
[[Code](https://github.com/IIGROUP/MAP)]

* **CLIP2Point: Transfer CLIP to Point Cloud Classification with Image-Depth Pre-training**, Tianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui Huang, Rynson W.H. Lau, Wanli Ouyang, Wangmeng Zuo 
[[Paper](https://arxiv.org/abs/2210.01055)] 
[[Code](https://github.com/tyhuang0428/CLIP2Point)]

* **F-VLM: OPEN-VOCABULARY OBJECT DETECTION UPON FROZEN VISION AND LANGUAGE MODELS**, Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, Anelia Angelova 
[[Paper](https://arxiv.org/pdf/2209.15639.pdf)] 
[[Code]()]


* **MEDICAL IMAGE UNDERSTANDING WITH PRETRAINED VISION LANGUAGE MODELS: A COMPREHENSIVE STUDY**, Ziyuan Qin, Huahui Yi, Qicheng Lao, Kang Li
[[Paper](https://arxiv.org/pdf/2209.15517.pdf)] 


* **ERNIE-VIL 2.0: MULTI-VIEW CONTRASTIVE LEARNING FOR IMAGE-TEXT PRE-TRAINING**, Bin Shan Weichong Yin Yu Sun Hao Tian Hua Wu Haifeng Wang, 
[[Paper](https://arxiv.org/pdf/2209.15270.pdf)]
[[Code](https://github.com/PaddlePaddle/ERNIE/)]


* **OmniVL: One Foundation Model for Image-Language and Video-Language Tasks**, Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, Lu Yuan 
[[Paper](https://arxiv.org/pdf/2209.07526.pdf)] 


* **Multi-Modal Masked Autoencoders for Medical Vision-and-Language Pre-Training**, Zhihong Chen, Yuhao Du, Jinpeng Hu, Yang Liu, Guanbin Li, Xiang Wan,and Tsung-Hui Chang, MICCAI-2022.
[[Paper](https://arxiv.org/pdf/2209.07098.pdf)]
[[Code](https://github.com/zhjohnchan/M3AE)]


* **EXPLORING VISUAL INTERPRETABILITY FOR CONTRASTIVE LANGUAGE-IMAGE PRE-TRAINING**, Yi Li, Hualiang Wang, Yiqun Duan, Hang Xu, Xiaomeng Li, 
[[Paper](https://arxiv.org/pdf/2209.07046.pdf)] 
[[Code](https://github.com/xmed-lab/ICLIP)]


* **PaLI: A Jointly-Scaled Multilingual Language-Image Model**, Xi Chen∗ Xiao Wang Soravit Changpinyo AJ Piergiovanni Piotr Padlewski, Daniel Salz Sebastian Goodman Adam Grycner Basil Mustafa Lucas Beyer, Alexander Kolesnikov Joan Puigcerver Nan Ding Keran Rong Hassan Akbari, Gaurav Mishra Linting Xue Ashish Thapliyal James Bradbury Weicheng Kuo, Mojtaba Seyedhosseini Chao Jia Burcu Karagol Ayan Carlos Riquelme, Andreas Steiner Anelia Angelova Xiaohua Zhai Neil Houlsby Radu Soricut 
[[Paper](https://arxiv.org/pdf/2209.06794.pdf)] 


* **CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment**, arxiv 2209.06430, Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, Jiebo Luo
[[Paper](https://arxiv.org/pdf/2209.06430.pdf)]
[[Code](https://github.com/microsoft/XPretrain/tree/main/CLIP-ViP)]

* RLIP: Relational Language-Image Pre-training for Human-Object Interaction Detection, Hangjie Yuan et al. [[Paper](https://arxiv.org/pdf/2209.01814.pdf)]

* **An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling**, Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, Zicheng Liu, [[Paper](https://arxiv.org/pdf/2209.01540.pdf)]

* **Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment**, Mustafa Shukor, Guillaume Couairon, Matthieu Cord, 
[[Paper](https://arxiv.org/pdf/2208.13628.pdf)] [[Code](https://github.com/mshukor/ViCHA)]

* **COYO-700M**: Image-Text Pair Dataset [[Paper]()] [[Code](https://github.com/kakaobrain/coyo-dataset)] 

* **Image as a Foreign Language: BEIT Pretraining for All Vision and Vision-Language Tasks**, Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others, arXiv:2208.10442, 2022. 
[[Paper](https://arxiv.org/pdf/2208.10442.pdf)] 
[[Code](https://aka.ms/beit-3)]

* **Pix4Point**: Image Pretrained Transformers for 3D Point Cloud Understanding, Guocheng Qian, Xingdi Zhang, Abdullah Hamdi, Bernard Ghanem 
[[Paper](https://arxiv.org/pdf/2208.12259.pdf)] 
[[Code](https://github.com/guochengqian/Pix4Point)]


* **VLMAE: Vision-Language Masked Autoencoder**, Sunan He, Taian Guo, Tao Dai, Ruizhi Qiao, Chen Wu, Xiujun Shu, Bo Ren, arXiv:2208.09374 [[Paper](https://arxiv.org/pdf/2208.09374.pdf)]

* Li, Juncheng, et al. "**Fine-Grained Semantically Aligned Vision-Language Pre-Training**." arXiv preprint arXiv:2208.02515 (2022). [[Paper](https://arxiv.org/pdf/2208.02515.pdf)]

* **GRIT-VLP: Grouped Mini-batch Sampling for Efficient Vision and Language Pre-training**, Jaeseok Byun, Taebaek Hwang, Jianlong Fu, and Taesup Moon, arXiv:2208.04060 
[[Paper](https://arxiv.org/pdf/2208.04060.pdf)]
[[Code](https://github.com/jaeseokbyun/GRIT-VLP)]

* Wang, Tengfei, et al. "**Pretraining is All You Need for Image-to-Image Translation**." arXiv preprint arXiv:2205.12952 (2022). [[Paper](https://arxiv.org/pdf/2205.12952.pdf)] [[Code](https://tengfei-wang.github.io/PITI/index.html)]

* Wang, Jinpeng, et al. "**Object-aware Video-language Pre-training for Retrieval.**" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. 
[[Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Object-Aware_Video-Language_Pre-Training_for_Retrieval_CVPR_2022_paper.pdf)] 
[[Code](https://github.com/FingerRec/OA-Transformer)]

* **See Finer, See More: Implicit Modality Alignment for Text-based Person Retrieval**, Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song, Ruizhi Qiao, Bo Ren, Xiao Wang, The 2nd Workshop on Real-World Surveillance: Applications and Challenges, ECCVW-2022  
[[Paper](https://arxiv.org/abs/2208.08608)] 
[[Code](https://github.com/shuxjweb/IVT)]

* **Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training**, 2022 European Conference on Computer Vision (ECCV 2022), Haoxuan You*, Luowei Zhou*, Bin Xiao*, Noel Codella*, Yu Cheng, Ruochen Xu, Shih-Fu Chang, Lu Yuan. 
[[Paper](https://arxiv.org/pdf/2207.12661.pdf)]
[[Code](https://github.com/Hxyou/MSCLIP)]

* Zhao, Tiancheng, et al. "**VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations.**" arXiv preprint arXiv:2207.00221 (2022). [[Paper](https://arxiv.org/pdf/2207.00221.pdf)] [[Code](https://github.com/om-ai-lab/VL-CheckList)]

* **DemoVLP: Revitalize Region Feature for Democratizing Video-Language Pre-training**, Guanyu Cai, Yixiao Ge, Alex Jinpeng Wang, Rui Yan, Xudong Lin, Ying Shan, Lianghua He, Xiaohu Qie, Jianping Wu, Mike Zheng Shou
[[Paper](https://arxiv.org/abs/2203.07720)]
[[Code](https://github.com/showlab/DemoVLP)]

* Yan, Rui, et al. "**Video-Text Pre-training with Learned Regions.**" arXiv preprint arXiv:2112.01194 (2021).
[[Paper]](https://arxiv.org/pdf/2112.01194.pdf)] 
[[Code](https://github.com/showlab/Region_Learner)]

* Wang, Alex Jinpeng, et al. "**All in one: Exploring unified video-language pre-training.**" arXiv preprint arXiv:2203.07303 (2022).
[[Paper](https://arxiv.org/pdf/2203.07303.pdf)]
[[Code](https://github.com/showlab/all-in-one)]

* **Egocentric Video-Language Pretraining**, Kevin Qinghong Lin and Alex Jinpeng Wang and Mattia Soldan and Michael Wray and Rui Yan and Eric Zhongcong Xu and Difei Gao and Rongcheng Tu and Wenzhe Zhao and Weijie Kong and Chengfei Cai and Hongfa Wang and Dima Damen and Bernard Ghanem and Wei Liu and Mike Zheng Shou, arXiv-2022 
[[Paper](https://arxiv.org/pdf/2206.01670.pdf)]
[[Code](https://github.com/showlab/EgoVLP)]

* **LocVTP: Video-Text Pre-training for Temporal Localization** (ECCV 2022), Meng Cao, Tianyu Yang, Junwu Weng, Can Zhang, Jue Wang and Yuexian Zou.  
[[Paper](https://arxiv.org/pdf/2207.10362.pdf)] 
[[Code](https://github.com/mengcaopku/LocVTP)]

* Gui L, Wang B, Huang Q, et al. **Kat: A knowledge augmented transformer for vision-and-language**[J]. arXiv preprint arXiv:2112.08614, 2021. 
[[**Paper**](https://arxiv.org/pdf/2112.08614.pdf)]
[[**Code**](https://github.com/guilk/KAT)] 



|**NO.**     | **Model**     | **Publish**        | **Modality**        | **Architecture**   | **Objective**     |**Highlights**   |**Code**         |
|:-----------|:-----------   |:----------------   |:----------------    |:----------------   |:----------------  |:----------------|:----------------|
|64 |pyramidCLIP |arXiv-2022  |image-text |CNN+Trans |CS|Hierarchical image-text contrastive learning|- 
|65 |VLC|arXiv-2022 |image-text  |ViT  |MIM, MLM ITM|Built on top of MAE that does not require trained on ImageNet|[[Code](https://github.com/guilk/VLC)]  
|66 |VLCDoC|arXiv-2022 |image-text  |Trans  |CS |Contrastive Pre-Training for document classification |-  
|67 |MVP |arXiv-2022  |image-text  |ViT  |MIM|Multimodality-guided visual pre-training leads to impressive gains |- 
|68 |COTS |arXiv-2022 |image-text  |Trans  |CS, KLD, MVLM|Token- and task-level interaction are proposed to enhance cross-modal interaction |-  
|69 |Flamingo |arXiv-2022 |image-text  |NFNet  |CS  |An architecture for accepting arbitrarily interleaved visual data and text as input |[[Code](https://github.com/lucidrains/flamingo-pytorch)]  
|70 |BLIP |arXiv-2022 |image-text |BERT  |CS, MML, MLM  |Propose the multimodal mixture of encoder-decoder, and captioning-filtering scheme |[[Code](https://github.com/salesforce/BLIP)]  
|71 |TCL |CVPR-2022 |image-text  |BERT  |CMA, IMC, LMI ITM, MLM  |The first work considers local structure information for multi-modality representation learning |[[Code](https://github.com/uta-smile/TCL)]  
|72 |SCALE |CVPR-2022 |image, text, table video, audio  |BERT  |MRP, MLM, MEM MFP, MFP, MAM|A unified model to handle five modalities  |[[Code](https://xiaodongsuper.github.io/M5Product_dataset/)]  
|73 |Clinical-BERT |AAAI-2022 |image-text  |BERT  |CD, MMM MLM, IMM  |The first work to learn domain knowledge during pre-training for the medical domain |- 
|74 |ProbES |ACL-2022 |image-text  |LSTM, ViLBERT  |Ranking loss |Prompt-based learning for VLN based on CLIP |[[Code](https://github.com/liangcici/Probes-VLN)]   
|75 |VLP-MABSA |ACL-2022 |image-text |BERT |MLM, AOE, MRM AOG, MSP |Task-specific VL-PTMs for multimodal aspect-based sentiment analysis |[[Code](https://github.com/NUSTM/VLP-MABSA )]
|76 |R2D2 |arXiv-2022 |image-text |ViT, BERT |GCPR, FGR, MLM |A two-way distillation strategy is proposed, i.e., target- and feature-guided distillation |- 
|77 |DeFILIP|arXiv-2022 |image-text |ViT, ResNet |CS |A benchmark for CLIP and its variants |[[Code](https://github.com/Sense-GVT/DeCLIP)]  
|78 |CoCa |arXiv-2022 |image-text |Trans |CS, ITG |Jointly pre-train image text model with contrastive loss and captioning loss |- 
|79 |HiVLP |arXiv-2022 |image-text |Trans |LRM, HRL, VLM |Accelerate image-text retrieval via hierarchical retrieval |- 
|80 |CLIP-Event |CVPR-2022 |image-text |Trans |CS |Consider event structural knowledge and prompts in the pre-training phase.|[[Code](https://github.com/limanling/clip-event)] 
|81 |AudioCLIP |ICASSP-2022 |image-text-audio |Trans |CS |Build a triplet modality based PTMs like CLIP |[[Code](https://github.com/AndreyGuzhov/AudioCLIP)] 
|82 |VL-BEiT |arXiv-2022 |image-text |Trans |MLM, MIM, MVLM |Pretrain on both monomodal and multimodal data using a shared Transformer |[[Code](https://github.com/microsoft/unilm)] 
|83 |MV-GPT |arXiv-2022 |image-text |BERT |MLM, LG |Pre-train both a multi-modal video encoder and a sentence decoder jointly. |- 
|84 |MMKD |arXiv-2022 |image-text |BERT |ITM |Iteratively execute knowledge discovery and model pre-training for continuous learning |-
|85 |GLIPv2 |arXiv-2022 |image-text |Swin, BERT |PGL, CS, MLM |Serves both the localization and understanding tasks. |[[Code](https://github.com/microsoft/GLIP)]
|86 |LIMoE |arXiv-2022 |image-text |Trans |CS |multi-modal pre-training with a sparse mixture of experts model |- 
|87 |VLMixer |arXiv-2022 |image-text |Trans |MLM, CMCL, MTM |Implicit cross-modal alignment learning in unpaired VLP. |[[Code](https://github.com/ttengwang/VLMixer)]
|88 |ProtoCLIP |arXiv-2022 |image-text |Trans |CS |Combine the CLIP loss and prototypical supervisions for VLP. |[[Code](https://github.com/megvii-research/protoclip)] 
|89 |i-Code |arXiv-2022 |image-text-audio |Trans |MLM, MVM MSM, CS |It can handle different combinations of modalities (such as single-, dual-, and triple-modality) into a single representation space. |- 





















### Year 2021 
|**NO.**     | **Model**     | **Publish**        | **Modality**        | **Architecture**   | **Objective**     |**Highlights**   |**Code**         |
|:-----------|:-----------   |:----------------   |:----------------    |:----------------   |:----------------  |:----------------|:----------------|
|25 |XGPT |NLPCC-2021 |image-text |Trans |IC, MLM, IDA, MOR |Novel IDA pre-training; Share parameters between encoder and decoder |- 
|26 |ERNIE-ViL |AAAI-2021 |image-text |Trans |MOC, AttP, RelP, MLM, MOR, MML |Use the knowledge obtained from scene graph |[[Code](https://github.com/Muennighoff/vilio)] 
|27 |KVL-BERT |KBS-2021 |image-text |BERT |MOC, MLM |Integrate commonsense knowledge for visual commonsense reasoning |- 
|28 |VinVL |CVPR-2021 |image-text |Trans |MTL, 3-way CS |Verifying that visual feature matters in VLP, i.e., strong object detector brings better results |[[Code](https://github.com/pzzhang/VinVL)] 
|29 |VL-T5 |ICML-2021 |image-text |Trans |MLM, VQA, MML, VG, GC |Unified framework for VL via generating texts |[[Code](https://github.com/j-min/VL-T5)] 
|30 |ViLT |ICML-2021 |image-text |Trans |MLM, MML |Use linear embedding only for Fast VL transformer|[[Code](https://github.com/dandelin/vilt)] 
|31 |ALIGN |ICML-2021 |image-text |EfficientNet, BERT |CS |Milestone for image-text pre-training using noisy data  |- 
|32 |Kaleido-BERT |CVPR-2021 |image-text |Trans |MLM, MML, AKPM |Use saliency detector to generate multi-grained patches |[[Code](http://dpfan.net/Kaleido-BERT)] 
|33 |MDETR |ICCV-2021 |image-text |CNN+Trans |STP, MML |An end-to-end text-modulated detection system |[[Code](https://github.com/ashkamath/mdetr)] 
|34 |SOHO |CVPR-2021 |image-text |CNN+Trans |MLM, MOR, MML|Use a dynamic-updated visual dictionary for vision-language alignment |[[Code](https://github.com/researchmm/soho)] 
|35 |E2E-VLP |ACL-2021 |image-text |Trans |OBD, ITG |The first end-to-end pre-trained model for V+L understanding and generation |- 
|36 |PIM |NeurIPS-2021  |image-text |Trans |MLM, MML, MOR |Propose a inter-modality flow metric to measure and reveal vision and language fusion |- 
|37 |CLIP-ViL |arXiv-2021 |image-text |Trans |MLM, VQA, MML |Take the CLIP visual encoder as its visual backbone |[[Code](https://github.com/clip-vil/CLIP-ViL)] 
|38 |ALBEF |NeurIPS-2021 |image-text |Trans |CS, GR |Design a momentum model to address noisy data |[[Code](https://github.com/salesforce/ALBEF)]	 
|39 |SimVLM |arXiv-2021 |image-text |Trans |PrefixLM |Simple VL model using single PrefixLM pre-training objective only |- 
|40 |MURAL |arXiv-2021 |image-text |Trans |CS |Adopt multi-task contrastive learning objective (image-text, text-text) |- 
|41 |VLMo |arXiv-2021 |image-text |Trans |MLM, MML, CS |Jointly learns visual-, text-encoder and a fusion encoder |[[Code](https://aka.ms/vlmo)] 
|42 |METER  |CVPR-2022  |image-text  |Trans  |MLM, MOR, MOC, MML |An empirical study on VLP  |[[Code](https://github.com/zdou0830/METER)] 
|43 |CLIP  |ICML-2021 |image-text  |Resnet, Trans |CS  |Milestone for image-text pre-training using noisy data |[[Code](https://github.com/OpenAI/CLIP)] 
|44 |Frozen  |ICCV-2021 |video/image-text  |Trans  |MML  |Flexibly trained on both images and videos with captions jointly  |[[Code](https://github.com/m-bain/frozen-in-time)]  
|45 |RegionLearner  |arXiv-2021 |video-text  |Trans  |MML  |Implicitly learning object region without position supervision  |[[Code](https://github.com/showlab/Region_Learner)]  
|46 |DALL-E |ICML-2021  |image-text  |Trans  |ELB  |Achieve high quality image generation without using any of the training labels  |[[Code](https://github.com/openai/DALL-E)]  
|47 |BriVL |arXiv-2021  |image-text  |Trans  |InfoNCE  |First large-scale Chinese multi-modal pre-training model |[[Code](https://github.com/chuhaojin/WenLan-api-document)]  
|48 |M6 |arXiv-2021 |image-text  |Trans  |LM  |The largest pretrained model in Chinese |- 
|49 |CogView |NeurIPS-2021  |image-text  |Trans  |NLL|The first open-source large text-to-image transformer  |[[Code](https://github.com/THUDM/CogView)]  
|50 |VATT |NeurIPS-2021  |Video, Audio, Text  |Trans  |NCE, MIL-NCE |Modality-specific or Modality-agnostic triplet modality pre-trained model |[[Code](https://github.com/google-research/google-research/tree/master/vatt)]  
|51 |OPT |arXiv-2021 |image, Audio, Text  |Trans  |MLM, MVM, MoLM MAM, DTR, DIR  |The first pre-trained model that connects the three modalities of text, vision, and audio  |-  
|52 |Florence |arXiv-2021 |image-text  |CoSwin  |UniCL|Expand the representations from coarse-to-fine, static-to-dynamic, and RGB-to-MM |-  
|53 |ROSITA |MM-2021 |image-text  |Trans  |SKM, MLM, MRM |Incorporates both cross- and intra-modal knowledge, and proposed SKM strategy  |-  
|54 |GilBERT |IR-2021 |image-text  |BERT  |MLM, MOR  |Employ image-to-text captioning and text-to-image synthesizing in VLP |-  
|55 |U-VisualBERT |NAACL-2021  |image-text |Trans, BERT  |GR, MML  |\emph{Unpaired image-text data for pre-training |[[Code](https://github.com/uclanlp/visualbert)]  
|56 |M3P |CVPR-2021 |image-text  |BERT  |xMLM, MC-MLM, MC-MRM  |Multitask, Multilingual, Multimodal Pre-training |[[Code](https://github.com/microsoft/M3P)] 
|57 |NUWA |arXiv-2021 |image-text  |Trans  |T2I, T2V, V2V  |A 3D transformer framework can handle image, text, and video, simultaneously |[[Code](https://github.com/microsoft/NUWA)]  
|58 |GLIP |CVPR-2022 |image-text  |BERT  |CS|Unifying detection and grounding by reformulating object detection as phrase grounding |[[Code](https://github.com/microsoft/GLIP)]  
|59 |RegionCLIP |CVPR-2022 |image-text  |Trans  |Distillation loss, CS  |Learn region-level visual representations based on CLIP  |[[Code](https://github.com/microsoft/RegionCLIP)]  
|60 |DeCLIP |ICLR-2022 |image-text |ViT |InfoNCE, SS MVS, NNS |Learn generic visual features in a data efficient way |[[Code](https://github.com/Sense-GVT/DeCLIP)] 
|61 |SLIP |arXiv-2021 |image-text |ViT |CS, InfoNCE|Combine the self-supervised learning and CLIP pre-training in a multi-task framework |[[Code](https://github.com/facebookresearch/SLIP)] 
|62 |FILIP |arXiv-2021 |image-text |ViT |CS|Achieve finer-level alignment using the cross-modal late interaction scheme |- 
|63 |SemVLP |arXiv-2021 |image-text |Trans |MLM, MOP, ITM, QA |Fuse the single- and two-stream architectures |- 






### Year 2020 
|**NO.**     | **Model**     | **Publish**        | **Modality**        | **Architecture**   | **Objective**     |**Highlights**   |**Code**         |
|:-----------|:-----------   |:----------------   |:----------------    |:----------------   |:----------------  |:----------------|:----------------|
|08 |Unicoder-VL |AAAI-2020 |image-text |Trans |GR, MML, MOC |Single transformer encoder for VLP |[[Code](https://github.com/microsoft/Unicoder)]
|09 |VLP |AAAI-2020 |image-text |Trans |BiDT, Seq2seq |Unified encoder-decoder network architecture |[[Code](https://github.com/LuoweiZhou/VLP)] 
|10 |UNITER |ECCV-2020 |image-text |Trans |MRA, MML |Propose an OT-based Word-Region Alignment objective |[[Code](https://github.com/ChenRocks/UNITER)]  
|11 |12-IN-1  |CVPR-2020  |image-text |Trans |CS, GR |Training jointly on 12 different datasets in a multi-task learning manner |[[Code](https://github.com/facebookresearch/vilbert-multi-task)] 
|12 |VisDial-BERT |ECCV-2020 |image-text |Trans |MLM, NSP, MIR |Pre-training on image-text corpus and finetuning on visual dialog |[[Code](https://github.com/vmurahari3/visdial-bert/)]
|13 |ImageBERT |arXiv-2020 |image-text |Trans |MOC, MLM, MML, MOR |Indicating that multi-stage pre-training works better |-  
|14 |PREVALENT |CVPR-2020 |image-text |Trans |MLM, AP |Pre-training for vision and language navigation |[[Code](https://github.com/weituo12321/PREVALENT)] 
|15 |InterBERT |arXiv-2020 |image-text |Trans |MSM, MOC, ITM-hn |Finding that all-attention works better than co-attention for modal interaction |[[Code](https://github.com/black4321/InterBERT)] 
|16 |PixelBERT |arXiv-2020 |image-text |CNN, Trans |MLM, MML |First to align vision and language in pixel and text-level |- 
|17 |OSCAR |ECCV-2020 |image-text |Trans |CS, MLM |Use object tags as anchor points to align image regions with word embeddings |[[Code](https://github.com/microsoft/Oscar)] 
|18 |FashionBERT |RDIR-2020 |image-text |BERT |MLM, MOR, MML |Use image patches for fashion domain instead of RoIs |[[Code](https://github.com/alibaba/EasyTransfer)] 
|19 |VILLA |NeurIPS-2020 |image-text |Trans |MLM, MOR, MML |Pre-training with adversarial learning |[[Code](https://github.com/zhegan27/VILLA)] 
|20 |UniVL  |arXiv-2020  |video-text  |Trans  |MLM, MFM, MML, ITG  |A unified model for multimodal understanding and generation  |[[Code](https://github.com/microsoft/UniVL)]  
|21 |HERO  |EMNLP-2020  |video-text  |Trans  |MLM, MFM, VSM, FOM |Hierarchical Transformer-based model trained with newly proposed VSM and FOM  |[[Code](https://github.com/linjieli222/HERO)] 
|22 |MMFT-BERT  |EMNLP-2020  |image-text  |BERT  |Classification |Adopt multiModal fusion Transformer for modality fusion|[[Code](https://github.com/aurooj/MMFT-BERT)] 
|23 |ActBERT  |CVPR-2020 |image-text  |Trans |CS, GR  |Extract actions explicitly as one of the inputs  |-  
|24 |UNIMO  |arXiv-2020 |image-text  |Trans  |CS  |Adapt to single-, multi-modal understanding and generation tasks effectively  |[[Code](https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO)]  






### Year 2019 and Before 
|**NO.**     | **Model**     | **Publish**        | **Modality**        | **Architecture**   | **Objective**     |**Highlights**   |**Code**         |
|:-----------|:-----------   |:----------------   |:----------------    |:----------------   |:----------------  |:----------------|:----------------|
|01 |VisualBERT |arXiv-2019 |image-text |Trans, BERT |GR, MML |A simple and strong baseline for VLP |[[Code](https://github.com/uclanlp/visualbert)]
|02 |ViLBERT |NeurIPS-2019 |image-text |Trans |CS, GR |First adopt co-attention for MM pre-training |[[Code](https://github.com/jiasenlu/vilbert_beta)] 
|03 |LXMERT |EMNLP-2019 |image-text |Trans | QA, MOR, MOC, MML, MLM |Propose a cross-modality encoder for vision-language pre-training |[[Code](https://github.com/airsplay/lxmert)] 
|04 |B2T2 |EMNLP-2019 |image-text |ResNet, BERT |MML, GR |Embed bounding box into text transformer in a early fusion manner |[[Code](https://github.com/google-research/language/tree/master/language/question_answering/b2t2)]  
|05 |VL-BERT |ICLR-2019 |image-text |BERT |GR, MOC |MM PTMs and faster rcnn are jointly trained |[[Code](https://github.com/jackroos/VL-BERT)] 
|06 |VideoBERT  |ICCV-2019 |video-text  |BERT  |MLM|A simple model for video-text feature learning |[[Code](https://github.com/ammesatyajit/VideoBERT)]  
|07 |CBT |arXiv-2019  |video-text  |Trans  |NCE  |Self-supervised contrastive bidirectional Transformer  |-  


















